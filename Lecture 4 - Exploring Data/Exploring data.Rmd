---
title: "Exploring Data"
author: "Dr. Liu Qizhang"
date: "24 August 2016"
output: html_document
---

Assuming you sign a big contract with an e-commerce company to conduct a market research for it. You have designed an online market survey form and sent out thousands of emails to invite consumers to do the survey for you. You also sent a team of assistants to conduct on-the-street survey for 2 weeks. Finally, data came in. You would like to dive directly into building models to analyse this set of data, so as to find consumer behavior patterns that your client is chasing you.

But WAIT! We have to understand the data first. It is extremely important, if not the most important, that we have to spend some time to explore the data for patterns and anomalies before jumping into model. We should never make assumption on the data and choose the model accordingly. Exploring data will help us select right models and save tremendous amount of time in trying models. 

Further to it, raw data is normally dirty. Human recording errors, typos, misinterpretation of survey questions, empty fields, and inconsistent data are common problems with raw data. We have to clean up the data before using it.

This lesson is about how to clean data, explore data and visualise data.

#Explore data with summary statistics

We have seen *str()* functon in previous lessons, which is used to compactly display the structure of an arbitrary R object. Applying *str()* on a matrix or data frame can give us some basic information about it. However, specific to data frame, *summary()* is more informatic and will give us a good overview of the data.

Please discuss about the findings given in the summary information on data set *custdata*:

```{r}
custdata<-read.table('custdata.tsv',header=T,sep = '\t')
summary(custdata)
```

The following are typical problems revealed by data summaries.

##Missing values

It is quite common that there will be missing values for some fields in a data set, especially if the data is collected manually or via an online survey, in which some fields are not compulsory. If there are just a small portion of missing values, then normally it is not a problem. We may just ignore them. However, if the percentage of missing values is high, then we have to handle them carefully. Many modeling algorithms in R will by default ignore those rows with missing values, which may significantly affect the validity of the model. The following are the suggested options to handle such cases.

###Missing data in categorical variables

For the mssing data in categorical variables, we cannnot try to *guess* the possible values, unless there is strong evidence that such data could be derived from other fields. A common practice is to create a dummy category in the variable and assign it to all the missing values.

Take *custdata* as an example. There is a huge amount of data without "Employed" status. We may create a new field and assign appropriate values to it.

```{r}
custdata$employment.status.fix<-ifelse(is.na(custdata$is.employed),"Unknown",
                                       ifelse(custdata$is.employed==T,"employed","unemployed"))
table(as.factor(custdata$employment.status.fix))
```


###Missing data in numeric variables

If the missing values are in numeric variables, we need to assign some numeric values to them. If you believe that the missing data is purely due to some random factors, then you can replace the missing values with the mean value of the variable.

```{r}
meanIncome<-mean(custdata$income, na.rm=T)
custdata$income.fix<-ifelse(is.na(custdata$income),meanIncome,custdata$income)
```

You may further improve this method. For example, if there is evidence that income is correlated to employment status, you may want to assign the missing value with the mean income of the corresponding employment status.

```
Exercise

Write codes to assign missing income with the mean income of the corresponding employment status.
```

If the values are missing systematically (not randomly), then the above method may not work. There are two ways to handle such case.

1. Convert the numeric data into categorical data, and then use the methods discussed before to fill up the missing categorical values. For example, we may divide the income into income levels, such as "Below 10,000", "10,000~20,000", etc.

2. You can also replace all the NAs with zero income. However, this zeros may be mixed up with the zeros already exist in the raw data. You may then add another field to keep track of which data points were originally missing.

```{r}
custdata$missingIncome<-is.na(custdata$income)
custdata$income.fix<-ifelse(is.na(custdata$income),0,custdata$income)
```

##Invalid values and outliers

Even when there is no missing data, it does not mean that it is good to proceed. Some of the data may not make sense (age of 100+), some of the data may be too extreme (outlier), and some data are not consistent. You will need to determine if such data was due to data entry error or just unusual data (we do have people live for more than 100 years).

You may need to decide what is the most appropriate action: drop the whole data field, drop those invalid data, or convert the bad data to useful value. Note that, even if outlier data are valid, you may still want to drop them because the goal of modeling is to predict the normal cases.

### Data Range

One way to check validity of data is to check its range (min value to max value). Let's take a look at the income range:

```{r}
summary(custdata$income)
```

The income has a very wide range, from -8700 to 615000. First of negative income may be invalid value, unless you allow it to represent debt. Secondly, the range is too wide. When data ranges over several orders of magnitude, it may cause problem for some modeling methods. Normally, a logarithm transformation is ideal, epsecially if data comes from *multiplicative process*. (Example, a 5% salary increasement to all staff, 20% discount to all goods in a store). And it is also easy to interpret the model based on logarithm transformation (a unit increase in log value can be interpreted as a percentage change in the original data).

In addition, monetary amounts - incomes, inventory, account, debts - are often lognormally distributed. Taking log of data can store symmetry to it, which will be useful for some modeling methods.

The following is an exmaple of how to log transform a data:

```{r}
logTran<-function(x) {ifelse(abs(x)<=1,0,sign(x)*log10(abs(x)))}
custdata$income.log <- logTran(custdata$income)
head(custdata[,c("income","income.log")], 10)
```

Sometimes, range may be too narrow. "narrow" here is a relative measurement. Age range from 0-2 is different from age range from 20-22. Sometimes, it may be useful to convert the unit of measurement to enlarge the range. For example, age range from 0-2 may be more meaningful to be converted into months.


#Explore data with graphics and visualization

Stephen Few, founder of Perceptual Edge, lists 8 core principles of data visualization:

* **Simplify** - Just like an artist can capture the essence of an emotion with just a few lines, good data visualization captures the essence of data - without oversimplifying.


* **Compare** - We need to be able to compare our data visualizations side by side. We can't hold the details of our data visualizations in our memory - shift the burden of effort to our eyes.

* **Attend** - The tool needs to make it easy for us to attend to the data that's really important. Our brains are easily encouraged to pay attention to the relevant or irrelevant details. Stephen demonstrated this convincingly with a video similar to Daniel Simon's classic gorilla and ball passing.

* **Explore** - Data visualization tools should let us just look. Not just to answer a specific question, but to explore data and discover things. Directed and exploratory analysis are equally valid, but we need to be sure that our visualization tool makes both possible.

* **View Diversely** - Different views of the same data provide different insights. It helps to be able to look at the same data from different perspectives at the same time and see how they fit together.

* **Ask why** - More than knowing "what's happening", we need to know "why it's happening". This is where actionable results come from.

* **Be skeptical** - We too rarely question the answers we get from our data because traditional tools have made data analysis so hard. We accept the first answer we get simply because exploring any further is tool hard. More powerful tools like Tableau give you the luxury to ask more questions, as fast as we can think of them.

* **Respond** - Simply answering questions for yourself has limited benefit. It's the ability to share our data that leads to global enlightenment.

"The best software for data analysis is the software you forget you're using. It's such a natural extension of your thinking process that you can use it without thinking about the mechanics." - Stephen Few

In this lesson, we are using R package "ggplot2" to demonstrate graphics and visualization.

##Distribution for a single variable

Histogram and density plot are two tools that can be used for us to visualize the distribution for a single variable. They are good at answering the following questions for us:

* What is the peak value of the distribution?
* How many peaks are there in the distribution (unimodality versus bimodality)?
* How normal is the data?
* How concentrated or dispersed is the data?

###Histogram

```{r}
library(ggplot2)

ggplot(custdata) + geom_histogram(aes(x=age), binwidth=5, fill="gray")
```

The problem with histogram is that its effect very much depends on the bin width. If it is too wide, then information will be lost. But if it is too narraw, then the histogram may appear to be too noisy.

###Density plots

```{r}
library(scales)

ggplot(custdata) + geom_density(aes(x=income)) + scale_x_continuous(labels=dollar)
```

We can logarithmix transform the data like the following:

```{r}
ggplot(custdata) + geom_density((aes(x=income))) +scale_x_log10(breaks=c(100,1000,10000,100000), label=dollar) + annotation_logticks(sides="bt")
```

The second graph look more "normalised".

###Box plot

*Box plot* is convenient way of graphically depicting groups of numerical data through their quartiles. It is a direct graphical representation of *summary* results.

```{r}
ggplot(custdata) + geom_boxplot(aes(x="Income",y=income))
```

This does not look good because of the extremely large outliers. There is a way for us to show it without outliers.

```{r}
#Compute the lower and upper whiskers
ylim = boxplot.stats(custdata$income)$stats[c(1, 5)]

# scale y limits based on ylim1
ggplot(custdata) + geom_boxplot(aes(x="Income",y=income)) + coord_cartesian(ylim = ylim*1.05)
```

###Bar chart

A *bar chart* is a histogram for discrete data (categorical values): it records the frequency of every value of a categorical variable. The following is a simple example:

```{r}
table(custdata$marital.stat)
ggplot(custdata) + geom_bar(aes(x=marital.stat), fill="blue")
```

As you can see, bar chart is just a graphical view of the *table()* result. Bar chart is most useful in visualizing trends when the number of possible values is large. For example:

```{r}
ggplot(custdata) + geom_bar(aes(x=state.of.res), fill="lightblue") + coord_flip() + theme(axis.text.y=element_text(size = rel(0.8)))
```

It may be more useful if the bars are sorted in the above bar chart. It will be a bit tedious to do so:

```{r}
#Obtain the statistics
stats<-as.data.frame(table(custdata$state.of.res))

#Change the default column names to  make them more meaningful
colnames(stats) <- c("state.of.res","count")

#Sort the data by count
statsf <- stats[order(-stats$count),]

#plot the bar chart
ggplot(statsf) + geom_bar(aes(x=state.of.res, y=count), stat="identity", fill="lightblue")+ coord_flip()+ theme(axis.text.y=element_text(size = rel(0.8)))
```

##Relationships between two variables

Very often, it is more useful and meaningful to visualize relationships between two variables. Some questions to answer from the visualization are:

* Is there relationship between two variables?
* Is the relationship strong?
* Is the relationship linear or not linear?

There are several charts for this purpose.

###Line plot

When two variables have *function like* relationship - just one y value for each x value - line plot is a good tool to visualise the relationship.

```{r}
x<-runif(100)
y<-asin(x)
ggplot(data.frame(x=x,y=y),aes(x=x,y=y) )+geom_line()
```

When the data does not have such function like relationship, line plots aren't really useful. 

```{r}
ggplot(custdata, aes(x=age, y=income)) + geom_line()
```

##Scatter plot

For normal cases, scatter plot is the first tool to have a good visualization of possible relationship between variables.

```{r}
ggplot(custdata,aes(x=age,y=income)) + geom_point() + ylim(-5000,200000)
```

You can fine tune this chart by removing some outlier or invalid values first.

```{r}
custdata2 <- subset(custdata, (custdata$age>0 & custdata$age<100 & custdata$income>0))
ggplot(custdata2,aes(x=age,y=income)) + geom_point() + ylim(0,200000)
```

We can add a linear trend line to the graph.

```{r}
ggplot(custdata2,aes(x=age,y=income)) + geom_point() + ylim(0,200000) + stat_smooth(method="lm")
```

This trend line does not seem to be meaningful. We can do better by plotting a smoothing curve through the data.

```{r}
ggplot(custdata2,aes(x=age,y=income)) + geom_point() + ylim(0,200000) + geom_smooth()
```

A scatter plot plus smoothing curve also is a good choice to view the relationship between a continuous variable and a Boolean. For example,

```{r}
ggplot(custdata2, aes(x=age, y=as.numeric(health.ins))) + geom_point(position=position_jitter(w=0.05,h=0.05)) + geom_smooth()
```

This graph shows that more people buying health insurance when becoming older.

##Hexbin plots

If the data volume grows bigger, scatter plot becomes more and more illegible smear. *Hexbin plot* - an aggregated version of scatter plot - turns out to be a good replacement.

```{r}
library(hexbin)

ggplot(custdata2, aes(x=age,y=income)) + geom_hex(binwidth=c(5,10000)) + geom_smooth(color="white",se=F) + ylim(0,200000)
```

##Bar charts for two categorical variables

Let's examine the relationship between marital status and the probability of health insurance coverage.

```{r}
#stacked bar chart allows us to compare across various categories
ggplot(custdata) + geom_bar(aes(x=marital.stat,fill=health.ins))
```

```{r}
#side-by-side chart is easier for comparison within categories
ggplot(custdata) + geom_bar(aes(x=marital.stat,fill=health.ins),position = "dodge")

#Filled bar chart is for comparison of ratios across different categories
ggplot(custdata) + geom_bar(aes(x=marital.stat,fill=health.ins),position = "fill")
```

To get a simultaneous sense of both the population in each category and the ratio of insured to uninsured, you can add what's called a *rug* to the filled chart. A rug is a series of ricks or points on the x-axis, one tick per datum. The rug is dense where you have a lot of data, and sparse where you have little data. 

```{r}
ggplot(custdata, aes(x=marital.stat)) + geom_bar(aes(fill=health.ins), position="fill") + geom_point(aes(y=-0.05), size=0.75,alpha=0.3,position=position_jitter(h=0.01))
```

```
Exercise

Adding rug to bar chart is not a very straightforward visual comparison. Please find a way to use a combination of stacked bar chart and line chart to have a better simultaneous view of both the population in each category and the ratio of insured to uninsured.
```

#Transforming Data

##Normalise data

Sometimes, we need to transform data to make it easier to model and easier to understand. For example, we cannot simply justify income level by its absolute value because the cost of living vary from place to place. Annual income of US$10,000 can barely support a living in Singapore, but it will be enviable in some other countries. Therefore, it makes sense for us to normalise data based on its context. 

For example, assuming we have a data frame named "medianincome" that stores median income by state in USA. Then the following codes can be used to normalise income data.

```
custdata<-merge(custdata, medianincome, by.x="state.of.res", by.y="State")
custdata$income.normalised <- with(custdata, income/Median.Income)
```

```
Exercise

Download median income by state data from https://en.wikipedia.org/wiki/List_of_U.S._states_by_income. Normalise income data in custdata based on the sample codes above. Then re-do the charts on income. Do you observe any difference?
```

We may normalise data by using mean and standard deviation.

```{r}
meanage <- mean(custdata$age)
stdage <- sd(custdata$age)
custdata$age.normalised <- (custdata$age-meanage)/stdage
summary(custdata$age.normalised)
```

##Converting continuous variables into discrete

For some continuous variables, their exact value matters less than whether they fall into a certain range. For example, you may notice that customers with income less than $20,000 behave differently in terms of purchasing insurance. Or customers younger than 25 and older than 65 have high probabilities of insurance coverage, because the younger ones tend to be on their parents' coverage and the older ones on a retirement plan.

The following graph shows a turning point when income level is $20,000. 

```{r}
#You can observe a kink at $20,000
ggplot(custdata2, aes(x=income, y=as.numeric(health.ins))) + scale_x_log10(breaks=c(100,1000,10000,100000), labels=dollar) + geom_point(position=position_jitter(w=0.05,h=0.05)) + geom_smooth()
```

In this case, you might want to divide the customers into two income groups: above $20,000 and below $20,000. Discreting continuous variables is useful when the relationship between input and output is not linear, but you are using a modeling technique that assume it is linear.

```{r}
#This is to divide income groups
custdata$income.group <- ifelse(custdata$income < 20000, "LOW", "HIGH")
custdata$income.group <- as.factor(custdata$income.group)
```

If you want to have more than just one simple threshold, you can use *cut()* function.

```{r}
#Divide age into multi groups
brks <- c(0,25,65,Inf)
custdata$age.group <- cut(custdata$age, breaks=brks, include.lowest = T)
summary(custdata$age.group)
```
